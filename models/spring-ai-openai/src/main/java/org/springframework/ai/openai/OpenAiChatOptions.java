/*
 * Copyright 2023 - 2024 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.springframework.ai.openai;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.List;
import java.util.Map;
import org.springframework.ai.model.function.PortableFunctionCallingOptions;
import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest.ResponseFormat;
import org.springframework.ai.openai.api.OpenAiApi.FunctionTool;
import org.springframework.ai.openai.api.OpenAiApi.ChatCompletionRequest.ToolChoiceBuilder;

/**
 * @author youngmon
 * @author Christian Tzolov
 * @since 0.8.0
 */
@JsonInclude(Include.NON_NULL)
public interface OpenAiChatOptions extends PortableFunctionCallingOptions {

	/**
	 * How many chat completion choices to generate for each input message. Note that you
	 * will be charged based on the number of generated tokens across all of the choices.
	 * Keep n as 1 to minimize costs.
	 */
	@JsonProperty("n")
	Integer getN();

	/**
	 * ID of the model to use.
	 */
	@JsonProperty("model")
	String getModel();

	/**
	 * An object specifying the format that the model must output. Setting to { "type":
	 * "json_object" } enables JSON mode, which guarantees the message the model generates
	 * is valid JSON.
	 */
	@JsonProperty("response_format")
	ResponseFormat getResponseFormat();

	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
	 * existing frequency in the text so far, decreasing the model's likelihood to repeat
	 * the same line verbatim.
	 */
	@JsonProperty("frequency_penalty")
	Float getFrequencyPenalty();

	/**
	 * Modify the likelihood of specified tokens appearing in the completion. Accepts a
	 * JSON object that maps tokens (specified by their token ID in the tokenizer) to an
	 * associated bias value from -100 to 100. Mathematically, the bias is added to the
	 * logits generated by the model prior to sampling. The exact effect will vary per
	 * model, but values between -1 and 1 should decrease or increase likelihood of
	 * selection; values like -100 or 100 should result in a ban or exclusive selection of
	 * the relevant token.
	 */
	@JsonProperty("logit_bias")
	Map<String, Integer> getLogitBias();

	/**
	 * The maximum number of tokens to generate in the chat completion. The total length
	 * of input tokens and generated tokens is limited by the model's context length.
	 */
	@JsonProperty("max_tokens")
	Integer getMaxTokens();

	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
	 * they appear in the text so far, increasing the model's likelihood to talk about new
	 * topics.
	 */
	@JsonProperty("presence_penalty")
	Float getPresencePenalty();

	/**
	 * This feature is in Beta. If specified, our system will make a best effort to sample
	 * deterministically, such that repeated requests with the same seed and parameters
	 * should return the same result. Determinism is not guaranteed, and you should refer
	 * to the system_fingerprint response parameter to monitor changes in the backend.
	 */
	@JsonProperty("seed")
	Integer getSeed();

	/**
	 * Up to 4 sequences where the API will stop generating further tokens.
	 */
	@JsonProperty("stop")
	List<String> getStop();

	/**
	 * A list of tools the model may call. Currently, only functions are supported as a
	 * tool. Use this to provide a list of functions the model may generate JSON inputs
	 * for.
	 */
	@JsonProperty("tools")
	List<FunctionTool> getTools();

	/**
	 * Controls which (if any) function is called by the model. none means the model will
	 * not call a function and instead generates a message. auto means the model can pick
	 * between generating a message or calling a function. Specifying a particular
	 * function via {"type: "function", "function": {"name": "my_function"}} forces the
	 * model to call that function. none is the default when no functions are present.
	 * auto is the default if functions are present. Use the {@link ToolChoiceBuilder} to
	 * create a tool choice object.
	 */
	@JsonProperty("tool_choice")
	String getToolChoice();

	/**
	 * A unique identifier representing your end-user, which can help OpenAI to monitor
	 * and detect abuse.
	 */
	@JsonProperty("user")
	String getUser();

}
